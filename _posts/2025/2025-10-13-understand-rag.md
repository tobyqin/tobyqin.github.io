---
title: 理解 RAG
categories: Tech
tags: engineering, ai
date: 2025-10-15
---

RAG 的核心思想，是将大语言模型 LLM 从一个封闭的“知识库”转变成一个开放的“信息处理器”。它通过解耦模型的参数化记忆（训练语料）和非参数化记忆（外部知识库），从根本上解决了LLM 知识固化、容易幻觉和缺乏溯源性的问题。

它本质上是将一个困难的开卷考试问题，转化成了一个阅读理解任务，不要求模型知道一切，只要求它根据提供的相关材料，进行归纳总结。

因此，RAG 系统能力的上限，并非由 LLM 单独决定，而是由检索器（上下文）和生成器（模型推理）这两大模块协同的短板所决定的。

模型可能会越来越强，但没有好的上下文，模型也只能扯淡。因此，优化 RAG 的精髓在于：

1. 原料决定品质：一切优化的基础来自于构建高质量的知识索引。这意味着必须进行精细化的内容感知分块和元数据处理，确保检索的内容是语义完整、干净无噪音的知识单元。

2. 猜心比听话更重要：不要指望用户的原始提问就是最近查询。通过查询转换和混合混合搜索，从语义和关键词等多个维度去主动猜测和逼近用户的真实意图，最大化召回的准确性和全面性。

3. 精选胜于海量：我们永远都需要回归到模型的注意力上，召回的终点不是给 LLM 提供尽可能多的文档，尽管上下文还没打满，提供最精确的上下文依然是优良结果的前提。引入重排环节，在送入 LLM 前加一道质检，确保喂给模型的是营养最丰富的精粮，而不是未经筛选的粗糠。
