---
title: 构建企业级 AI Code Review 协作体系
categories: Tech
tags: ai
date: 2025-11-15
---

在座的每一位工程领袖和开发者，都在与 Code Review (CR) 的“不可能三角”作斗争：**我们渴望速度，追求质量，但又受限于有限的资深工程师时间。**

传统的 Linter 解决了“风格”；静态分析 (SAST) 解决了“已知漏洞”。但谁来解决“架构一致性”？谁来检查“业务意图”？谁来发现那个“重复造轮子”的函数？

我们尝试过“天真”的 AI——将整个 Diff 喂给一个 LLM，得到的往往是“看起来不错”或“变量名不规范”这类浅层反馈。它们是“代码警察”，不是“工程伙伴”。

为什么？因为它们**缺乏上下文**。它们不知道你上周修复的那个 Bug，不知道你们团队的设计规范，更不知道这个 CR 背后那个 Jira 里的“真正原委”。

这篇文章将论述一套完整的蓝图：如何构建一个**集成了“工程上下文”**、**模拟“专家委员会”**、并能**自我进化**的 AI Code Review 协作体系。

---

### 阶段一：重新定义问题，AI 评审的核心挑战

我们的目标不是构建一个“全能 AI”，而是构建一个“**自动化评审委员会**”。我们首先要承认，一个“天真”的 AI 评审是失败的，因为它无法回答三个核心问题：

1.  **“原委”问题：** AI 不知道这个 CR **为什么**存在。它看不到 Jira 里产品经理的讨论和架构师的决定。
2.  **“边界”问题：** AI 无法理解“领域边界”。它可能会因为 `Project A` 的代码与 `Project B` 相似而报警，却不知道这在组织上是故意为之。
3.  **“失焦”问题：** 一个试图同时检查安全、架构、性能和风格的“全能 AI”，最终会失去重心，给出笼统或无关紧要的建议。

因此，我们的设计必须从根本上解决这三点：**集成意图**、**尊重边界**、**拆分人设**。

---

### 阶段二：技术选型，打造 AI 的“大脑”与“记忆”

要让 AI 像资深工程师一样思考，就必须给它一个“大脑”（LLM）和一套“长期记忆”（RAG 索引）。

#### 1. 大脑 (LLM)：混合专家模型

我们不应只依赖一个模型。我们的**整个流水线**就是一种“专家混合”（Mixture of Experts）。

* **Gemini 1.5 Flash：** 用于“快速、廉价”的任务。它是我们的“分诊台机器人”，用于分析 CR、路由任务、检查简单规范。
* **Gemini 1.5 Pro：** 用于“昂贵、深度”的任务。它是我们的“资深架构师”和“安全官”，负责需要复杂推理的深度评审。

#### 2. 记忆 (RAG)：构建“工程上下文”四大索引

这是整个系统的基石。我们的 RAG **不是一个索引，而是四个**：

* **索引 A：代码实现 (The "How")**
    * **数据：** 5000 个代码库的源码，使用**基于 AST (抽象语法树)** 的切分器，按函数/类进行向量化。
    * **目的：** 检查代码冗余、发现“重复造轮子”。

* **索引 B：Git 演进 (The "Evolution")**
    * **数据：** 所有库的 Git Commit 历史。
    * **目的：** 让 AI 知道“这段代码为什么上次被修改”，避免破坏关键修复。

* **索引 C：规范与文档 (The "Law")**
    * **数据：** `READMEs`、`DESIGN_DOCS.md`、Confluence Wiki、以及**公司级的《代码规范》、《安全红线》、《API 设计指南》**。
    * **目的：** 为评审提供“法律依据”。

* **索引 D：意图与原委 (The "Why")**
    * **数据：** Jira / GitHub Issues 的票据标题、描述、评论。
    * **目的：** 解决“原委”问题，让 AI 知道 CR 的**真正业务意图**。

#### 3. 数据库 (Vector DB)：为什么 `pgvector` 是优选？

我们对比了专业向量库（Qdrant, Milvus）和“一体化”方案。在这个场景下，**pgvector** (PostgreSQL 扩展) 展现出巨大优势：

* 我们的场景需要**极强的元数据过滤**（例如：`WHERE repo_name = 'X'` 和 `project = 'Y'`）与向量搜索结合。
* 我们需要用 SQL `JOIN` 来连接代码、Jira 和 Git 历史。
* `pgvector` 让我们能在**同一个事务**中管理所有数据，极大地降低了架构复杂性。**Elasticsearch** 凭借其强大的“混合搜索”（关键词 + 向量）是另一个值得考虑的有力竞争者。

---

### 阶段三：设计实现，“AI 评审委员会”流水线

这是我们的核心逻辑。一个 CR 不再是一个单一任务，而是一个四阶段的自动化流水线。

#### 阶段 1：分诊台 (Triage Bot)
* **触发：** CR (Pull Request) 提交时。
* **模型：** Gemini Flash。
* **动作：**
    1.  **解析意图：** 强制从 Commit Message 中解析 `[PROJ-123]`。
    2.  **分析 DNA：** 读取 Diff 统计（`+500行` = 新功能）和文件类型（改了 `.sql`？改了 `auth/`？）。
    3.  **激活人设：** 根据 DNA，动态激活一个或多个“专家评审” (`@安全专家`, `@DBA专家`)。

#### 阶段 2：RAG 备课
* 系统**并行地**为每位“专家”准备定制化的“上下文包”。
* `@安全专家` 得到：`PROJ-123` 意图 + `安全红线规范` (索引 C) + `相似安全代码` (索引 A)。
* `@架构专家` 得到：`PROJ-123` 意图 + `架构设计文档` (索引 C) + `该文件的 Git 历史` (索引 B)。

#### 阶段 3：并行评审 (人设拆分)
* 我们**并行发起多个、小而精的 LLM 调用**，每个调用都有一个高度专注的 System Prompt。
* **`@安全专家` (Gemini Pro):**
    * *Prompt:* “你只关心安全。忽略风格。”
    * *产出:* “【安全-高危】第 50 行有 SQL 注入风险...”
* **`@架构专家` (Gemini Pro):**
    * *Prompt:* “你只关心架构一致性。忽略安全细节。”
    * *产出:* “【架构-建议】此改动违反了‘支付项目’的设计规范，角色应从 Role-Service 动态注入...”
* **`@规范专家` (Gemini Flash):**
    * *Prompt:* “你只关心代码风格和命名。”
    * *产出:* “【规范-低】第 12 行函数名 `getUser` 应为 `get_user`...”

#### 阶段 4：综合与反馈
* 一个“**AI 组长**” (Gemini Pro) 登场。
* **动作：**
    1.  汇总所有专家报告。
    2.  按严重性排序（安全 > 架构 > 规范）。
    3.  智能去重。
    4.  以友善的 Tech Lead 口吻，发布**一条单一的、清晰的、可执行的** CR 评论。

---

### 阶段四：优化思考，让系统更“聪明”

一个好的系统会工作；一个伟大的系统会进化。

#### 1. 优化：“领域边界” (Hierarchical Scoping)
我们必须解决“A 项目 vs B 项目”的干扰问题。搜索不应是全局的：

* **L1 (最高优先级): Repo-Local**。只在 CR 所在的**仓库内**搜索。
* **L2 (中优先级): Project-Local**。如果 L1 未命中，扩展到该仓库所属的**“项目”**（例如 `Project "Payments"` 下的所有 Repo）。
* **L3 (低优先级): Shared Kernel**。最后，才去搜索“公司级共享库”（如 `company-utils`）。

#### 2. 优化：“焦点评审” (Iterative Review)
假如一个 CR 会有 3 次迭代。我们不能每次都“完整评审”。

* **Iteration 1：** 运行“完整委员会”。
* **Iteration 2 (修改稿)：** “分诊台”介入，对比新 Diff 和“AI 委员会”的上一份报告。
* **智能激活：** “哦，开发者只改了 `@安全专家` 提到的那 3 行代码。”
* **动作：** **只激活 `@安全专家`**，并给它一个新 Prompt：“请确认你上次提的 SQL 注入问题已被修复。”
* **成本影响：** 迭代评审的成本**降低 50%-60%**。

#### 3. 优化：“反馈闭环” (Self-Healing)
* **正向激励：** 当开发者点击“采纳此建议”时，这个 (Diff -> 建议 -> 采纳) 三元组被存回 RAG 索引，作为“**优秀评审案例**” (Golden Example)。
* **负向激励：** 当开发者回复“你这个建议是错的，因为...”，这个“**人类反驳**” (Human Rebuttal) 也会被索引，作为“反面教材”，防止 AI 下次再犯。

---

### 阶段五：容量与成本，这到底要花多少钱？

让我们把这个愿景落地到数字上。

#### 1. 容量规划
* **场景：** 每月 40,000 个 CR。
* **反推团队规模：** 40,000 CRs / 20 工作日 = 2,000 CRs/天。这指向一个**约 1,500 人**的工程团队。
* **反推 Repo 活跃度：** 5,000 个库中，可能只有 **1,000 个是“热”库**，它们产生了绝大多数 CR。
* **RAG 索引规模：** 基于 5,000 库（包括历史），我们的总向量规模在“近亿级” (7,500 万+)，总数据（向量+元数据）在 **TB 级**。这再次印证了 `pgvector` 或 `ES` 这样能处理大容量、强过滤的数据库的必要性。

#### 2. 成本计算 (使用 Gemini API)
我们基于“混合策略”和“焦点评审”进行估算：

* **单次 CR 成本 (Iteration 1 - 完整评审)：**
    * 涉及 3 个专家（2 Pro + 1 Flash）和 RAG 上下文。
    * 估算消耗：~32k Input Tokens (Pro), ~8k Input Tokens (Flash)
    * **成本：约 $0.145 / 每次**

* **单次 CR 成本 (Iteration 2/3 - 焦点评审)：**
    * 只激活 1 个专家 (Pro)。
    * **成本：约 $0.0725 / 每次**

* **月度总成本 (40,000 个 CR，平均 3 次迭代)：**
    * `事件 1 (初始提交)`: 40,000 ✕ $0.145 = $5,800
    * `事件 2/3 (迭代提交)`: 80,000 ✕ $0.0725 = $5,800
    * **总计：约 11,600 美元 / 月**

**12,000 美元，对于一个 1,500 人的工程团队，这连一名初级工程师月薪的 1/3 都不到。**

### 结论：规模化你的工程智慧

我们论证了一条清晰的路径。通过**集成四大“工程上下文”索引**，设计“**专家委员会**”流水线，并实现“**焦点评审**”和“**反馈闭环**”，我们能以极低的成本，构建一个真正强大的 AI 工程伙伴。

这不再是科幻。它的价值也不是“取代”工程师，而是“规模化”你最资深工程师的智慧——让团队中的每一个人，都能像拥有十年经验的架构师一样，去思考、去评审。
